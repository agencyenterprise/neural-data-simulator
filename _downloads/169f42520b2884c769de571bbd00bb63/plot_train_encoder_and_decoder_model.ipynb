{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Train models for the encoder and decoder\n\n.. currentmodule:: neural_data_simulator\n\nThis example illustrates the steps required to train and save models\nfor the NDS encoder and decoder. The encoder model is used to convert\nvelocity into spiking rates, while the decoder model is used to predict\nvelocity from spiking rates.\n\nThe models will be trained using a slice of data from the dataset\n[Structure and variability of delay activity in premotor cortex](https://dandiarchive.org/dandiset/000121) and [paper with the same title](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006808),\nspecifically the data from `Session 4` with the subject `JenkinsC`.\n\nWe mostly followed [decoding arm speed during reaching](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6286377/pdf/41467_2018_Article_7647.pdf)\nfor modeling. One reason to follow this paper specifically is that we wanted\nto use a model that incorporated the magnitude of the velocity as well as its\ndirection. Traditional cosine tuning curves only include direction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download data\nRetrieve the preprocessed training data from AWS S3 and validate its checksum.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urljoin\n\nimport pooch\n\nDOWNLOAD_BASE_URL = \"https://neural-data-simulator.s3.amazonaws.com/sample_data/v1/\"\n\nBEHAVIOR_DATA_PATH = pooch.retrieve(\n    url=urljoin(DOWNLOAD_BASE_URL, \"session_4_behavior.npz\"),\n    known_hash=\"md5:fff727f5793f62c0bf52e5cf13a96214\",\n)\n\nSPIKES_TRAIN_DATA_PATH = pooch.retrieve(\n    url=urljoin(DOWNLOAD_BASE_URL, \"session_4_spikes_train.npz\"),\n    known_hash=\"md5:6ba47e77c3045c1d4eda71f892a5cdc3\",\n)\n\nSPIKES_TEST_DATA_PATH = pooch.retrieve(\n    url=urljoin(DOWNLOAD_BASE_URL, \"session_4_spikes_test.npz\"),\n    known_hash=\"md5:9df38539d794f91049ddc23cc5e04531\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data\nLoad the training and test data from the downloaded files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\n\nbehavior_data = np.load(BEHAVIOR_DATA_PATH)\nspikes_train_data = np.load(SPIKES_TRAIN_DATA_PATH)\nspikes_test_data = np.load(SPIKES_TEST_DATA_PATH)\n\ntimestamps = behavior_data[\"timestamps_train\"]\nvel_train = behavior_data[\"vel_train\"]\nvel_test = behavior_data[\"vel_test\"]\nspikes_train = spikes_train_data[\"spikes_train\"]\nspikes_test = spikes_test_data[\"spikes_test\"]\n\nassert vel_train.shape[0] == spikes_train.shape[0]\n\nplt.plot(timestamps, vel_train)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove outliers\nWe observe that at least one sample is an outlier,\nso let's remove all outliers from the dataset before\ntraining the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_indices_without_outliers(data, n=9):\n    std_dev = np.std(data)\n    mean = np.mean(data)\n    return np.where(np.any(abs(data - mean) < n * std_dev, axis=1))\n\n\nvel_train_without_outliers_indices = get_indices_without_outliers(vel_train)\nvel_train = vel_train[vel_train_without_outliers_indices]\nspikes_train = spikes_train[vel_train_without_outliers_indices]\ntimestamps = timestamps[vel_train_without_outliers_indices]\n\nassert vel_train.shape[0] == spikes_train.shape[0]\n\nplt.plot(timestamps, vel_train)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standardize velocity\nStandardize velocity horizontal and vertical directions by\nscaling to unit variance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(vel_train)\n\nvel_train = scaler.transform(vel_train)\nvel_test = scaler.transform(vel_test)\n\nprint(\"scale =\", scaler.scale_)\nprint(\"mean =\", scaler.mean_)\nprint(\"var =\", scaler.var_)\n\nplt.plot(timestamps, vel_train)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export standardized model for streaming\nFor replaying the data with the NDS streamer, keep all samples\nfrom the dataset, but clip them, then apply the scaler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nfrom neural_data_simulator.util.runtime import get_sample_data_dir\n\nSTANDARDIZED_BEHAVIOR_DATA_PATH = os.path.join(\n    get_sample_data_dir(), \"session_4_behavior_standardized.npz\"\n)\nVELOCITY_LIMIT = 20\n\nstr_timestamps = behavior_data[\"timestamps_train\"]\nstr_vel_train = np.clip(behavior_data[\"vel_train\"], -VELOCITY_LIMIT, VELOCITY_LIMIT)\nstr_vel_train = scaler.transform(str_vel_train)\nstr_vel_test = np.clip(behavior_data[\"vel_test\"], -VELOCITY_LIMIT, VELOCITY_LIMIT)\nstr_vel_test = scaler.transform(str_vel_test)\n\nnp.savez(\n    STANDARDIZED_BEHAVIOR_DATA_PATH,\n    timestamps_train=str_timestamps,\n    vel_train=str_vel_train,\n    vel_test=str_vel_test,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create tuning curves\nWe're ready to start creating the encoder model. We'll follow the\nequation from [decoding arm speed during reaching](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6286377/pdf/41467_2018_Article_7647.pdf):\n\n$spike rate = b_0 + m * |v| * cos(\\theta - \\theta_{pd}) + b_s*|v|$\n\nwhere `spike rate` is the spike data for the unit we're trying to model,\n$b_0$, $m$, $\\theta_{pd}$, and $b_s$ are the coefficients\nwe're fitting $\\theta_{pd}$ is the preferred direction for this unit,\n$\\theta$ is the direction of the velocity and $|v|$ is the magnitude of\nthe velocity.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>There is a time delay component removed from the original equation. In the paper,\n   the authors searched for the optimal time delay for each unit. It's expected that\n   we'll see spikes happening before the actual movement. For decoding kinematics\n   from spikes, that's a helpful delay, because you have extra time to process the\n   current spikes as the motor action is only coming in a few milliseconds; but in\n   our case, this means we'd need to use data from the future -- or in practice,\n   we'd need to add an extra delay between input and output, which is not desired.\n   So for this initial model, we'll use \"current\" behavior data to create \"current\"\n   spikes.</p></div>\n\nLet's fit one model for each unit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import curve_fit\n\n\ndef tuning_model(x, b0, m, pd, bs):\n    \"\"\"\n    Spiking rate tuning curve from the paper decoding arm speed during reaching\n    \"\"\"\n    mag_vel = np.linalg.norm(x, axis=1)\n    theta = np.arctan2(x[:, 1], x[:, 0])\n    return b0 + m * mag_vel * np.cos(theta - pd) + bs * mag_vel\n\n\nclass UnitModel:\n    \"\"\"\n    Stores key data and model for each unit in our simulation\n    \"\"\"\n\n    def __init__(self, tuning_model, unit_id):\n        self.id = unit_id\n        self.model_params = []\n        self.model_error = []\n        self.model = tuning_model\n\n        self.spike_rates_to_fit = None\n        self.velocity_to_fit = None\n\n    def fit(self, velocity, spike_rates):\n        \"\"\"\n        Find model parameters to spike rates to the velocity using the tuning model\n        \"\"\"\n        self.model_params, _ = curve_fit(self.model, velocity, spike_rates)\n        self.spike_rates_to_fit = spike_rates\n        self.velocity_to_fit = velocity\n\n    def evaluate(self, velocity):\n        \"\"\"\n        Get rates based on model fit in the fit function to the given velocities\n        \"\"\"\n        rates = self.model(velocity, *self.model_params)\n        rates[rates <= 0] = 1e-9\n        return rates\n\n\nunits = []\nfor unit_id in range(spikes_train.shape[1]):\n    unit_model = UnitModel(tuning_model, unit_id)\n    unit_model.fit(vel_train, spikes_train[:, unit_id])\n    units.append(unit_model)\n\nparams = np.array([u.model_params for u in units])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get spiking data from models\nIn this section, we'll follow a nomenclature similar to that from the NLB\nchallenge, where `rates` are the expected number of spikes in a bin -- this\nis what we get out of our original equation when we create our tuning\ncurve model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "simulated_rates_train = np.zeros_like(spikes_train)\nfor i, u in enumerate(units):\n    simulated_rates_train[:, i] = u.evaluate(vel_train)\n\nsimulated_rates_test = np.zeros_like(spikes_test)\nfor i, u in enumerate(units):\n    simulated_rates_test[:, i] = u.evaluate(vel_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at what our simulated data looks like compared to the\noriginal smoothed binned spiking data to a few randomly chosen units.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy import signal\n\nnp.random.seed(20230111)\n\nBIN_WIDTH = 0.02\nRANGE_TO_PLOT = slice(2000, 3000)\n\n\ndef smooth_spike_counts(X, *, bin_ms=5, sd_ms=40):\n    \"\"\"\n    Smooth the spike counts using a Gaussian filter\n    \"\"\"\n    kern_sd = int(round(sd_ms / bin_ms))\n    window = signal.gaussian(kern_sd * 6, kern_sd, sym=True)\n    window /= np.sum(window)\n    filt = lambda x: np.convolve(x, window, \"same\")\n    return np.apply_along_axis(filt, 1, X)\n\n\nunits_to_plot = np.random.choice(np.arange(len(units)), 16)\n\nplt.figure(figsize=(15, 10))\nfor i, u in enumerate(units_to_plot):\n    plt.subplot(4, 4, i + 1)\n\n    # we need to smooth the data after sampling from the poisson\n    ys = smooth_spike_counts(\n        simulated_rates_train[:, u].reshape(1, -1),\n        bin_ms=BIN_WIDTH,\n        sd_ms=3 * BIN_WIDTH,\n    )\n    plt.plot(ys[0, RANGE_TO_PLOT], label=\"Simulated (rates)\")\n    plt.plot(spikes_train[RANGE_TO_PLOT, u], alpha=0.5, label=\"Original\")\n\n    plt.ylabel(\"Rates (spk/sec)\")\n    plt.xlabel(\"Time bin\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the baseline for each of the rates is nonzero, which is the\nspiking rate when no movement is being performed.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate results\nTo get an idea of how our simulated data is performing, let's use the\nevaluation tools from the NLB challenge.\n\nFor more information about these metrics, take a look at the [NLB challenge\nevaluation page](https://eval.ai/web/challenges/challenge-page/1256/evaluation)\nor at their paper [Neural Latents Benchmark '21: Evaluating latent variable\nmodels of neural population activity](https://arxiv.org/pdf/2109.04463.pdf).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cobps\nCo-smoothing (`cobps`) was the main metric used for the challenge which is\nbased on the log-likelihood of neurons activity. `cobps` would be 0 if the\npredicted rates were the average of the actual spike rate. The higher `cobps`\nthe better.\n\nIt's hard to interpret `cobps` though, as you can't infer from the `cobps`\nresults from other datasets. Here, we calculate a few different `cobps` for\nthis dataset to try to get a better sense of how well we're doing.\nHowever, this will be more useful as we create new models as this will\nallow us to tell whether our models are improving.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nlb_tools.evaluation import bits_per_spike\n\nr = simulated_rates_train.reshape(\n    1, simulated_rates_train.shape[0], simulated_rates_train.shape[1]\n)\ns = spikes_train.reshape(1, spikes_train.shape[0], spikes_train.shape[1])\n\nprint(f\"cobps for original data (i.e. best achievable): {bits_per_spike(s, s)}\")\nprint(\n    \"cobps for mean per channel (this should be 0 by the co-bps definition): \"\n    f\"{bits_per_spike(np.ones_like(s)*np.mean(s, axis=1), s)}\"\n)\nprint(\n    \"cobps for mean across all training data: \"\n    f\"{bits_per_spike(np.ones_like(s)*np.mean(s), s)}\"\n)\nprint(\n    \"cobps for random uniform data: \"\n    f\"{bits_per_spike(np.random.uniform(np.min(s), np.max(s), size=s.shape), s)}\"\n)\nprint(f\"cobps for simulated data (train data): {bits_per_spike(r, s)}\")\n\nr = simulated_rates_test.reshape(\n    1, simulated_rates_test.shape[0], simulated_rates_test.shape[1]\n)\ns = spikes_test.reshape(1, spikes_test.shape[0], spikes_test.shape[1])\nprint(f\"cobps for simulated data (test data): {bits_per_spike(r, s)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## $R^2$ of behavioral data\nAnother way that the quality of generated spikes was evaluated for the NLB\nchallenge was using them on ridge regression model trained with actual data.\n\nHere we'll do a similar approach, but we'll try different combinations of\ntraining/testing with real or simulated data to try to extract more information.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nlb_tools.evaluation import fit_and_eval_decoder\n\nr2 = fit_and_eval_decoder(spikes_train, vel_train, spikes_test, vel_test)\nprint(f\"trained on real data, tested on real data: {r2}\")\nr2 = fit_and_eval_decoder(spikes_train, vel_train, simulated_rates_test, vel_test)\nprint(f\"trained on real data, tested on simulated data: {r2}\")\nr2 = fit_and_eval_decoder(simulated_rates_train, vel_train, spikes_test, vel_test)\nprint(f\"trained on simulated data, tested on real data: {r2}\")\nr2 = fit_and_eval_decoder(\n    simulated_rates_train, vel_train, simulated_rates_test, vel_test\n)\nprint(f\"trained on simulated data, tested on simulated data: {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export the encoder model to a file\nSave the velocity tuning curve parameters to a file that can be used\nin the encoder configuration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ENCODER_MODEL_PATH = os.path.join(\n    get_sample_data_dir(), \"session_4_tuning_curves_params.npz\"\n)\n\nparams = np.array([u.model_params for u in units])\n\nnp.savez(\n    ENCODER_MODEL_PATH,\n    b0=params[:, 0],\n    m=params[:, 1],\n    pd=params[:, 2],\n    bs=params[:, 3],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a model for the decoder\nFit a model to predict velocity from the spiking data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n\ndecoder = Ridge()\ndecoder.fit(spikes_train, vel_train)\nreal_score = decoder.score(spikes_test, vel_test)\nsim_score = decoder.score(simulated_rates_test, vel_test)\n\nprint(f\"trained on real data, tested on real spikes: {real_score}\")\nprint(f\"trained on real data, tested on simulated spikes: {sim_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export the decoder model to a file\nDump the model to a file that can be used in the decoder configuration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import joblib\n\nDECODER_MODEL_PATH = os.path.join(\n    get_sample_data_dir(), \"session_4_simple_decoder.joblib\"\n)\n\njoblib.dump(decoder, DECODER_MODEL_PATH)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}